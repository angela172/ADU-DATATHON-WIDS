{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats.mstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r'C:\\Users\\11 PrO\\Downloads\\train.csv\\train.csv')\n",
    "df_test  = pd.read_csv(r'C:\\Users\\11 PrO\\Downloads\\test.csv\\test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing redundant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_redundant_columns(dataset):\n",
    "    # Create a set to store the names of columns already encountered\n",
    "    seen = set()\n",
    "    # List to store columns to drop\n",
    "    to_drop = []\n",
    "\n",
    "    # Iterate through columns\n",
    "    for column in dataset.columns:\n",
    "        # If we have already seen the column name, mark it for dropping\n",
    "        if column in seen:\n",
    "            to_drop.append(column)\n",
    "        else:\n",
    "            seen.add(column)\n",
    "\n",
    "    # Drop redundant columns\n",
    "    dataset.drop(to_drop, axis=1, inplace=True)\n",
    "    return dataset\n",
    "\n",
    "# Example usage:\n",
    "# Load your dataset\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Remove redundant columns\n",
    "data1 = remove_redundant_columns(df_train)\n",
    "data2 = remove_redundant_columns(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking for duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows:\n",
      "Empty DataFrame\n",
      "Columns: [id, CustomerGender, CustomerType, Age, TravelPurpose, ClassTravelled, DistanceToDestination, Inflight wifi, Time convenience, WebsiteExperience, ConvenienceOfGate, Inflight Food, Online check-in, ComfortOfSeats, Inflight entertainment system, On-board service, Leg room in flight, Baggage handling ease, Checkin service, Inflight service, Cleanliness, Departure Delay in Minutes, Arrival Delay in Minutes, CustomerHappiness]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming data1 is your DataFrame\n",
    "duplicate_rows = data1[data1.duplicated()]\n",
    "\n",
    "# Display duplicate rows\n",
    "print(\"Duplicate Rows:\")\n",
    "print(duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 35\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_data1\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Assuming 'data1' is your data1set (a numpy array or pandas data1Frame)\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m cleaned_data1 \u001b[38;5;241m=\u001b[39m \u001b[43mremove_outliers_iqr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m, in \u001b[0;36mremove_outliers_iqr\u001b[1;34m(data1, threshold)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mRemove outliers from a data1set using the Interquartile Range (IQR) method.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    numpy array or pandas data1Frame: The data1set with outliers removed.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Calculate the first quartile (Q1) and third quartile (Q3)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m Q1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpercentile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m Q3 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(data1, \u001b[38;5;241m75\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate the interquartile range (IQR)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\11 PrO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:4283\u001b[0m, in \u001b[0;36mpercentile\u001b[1;34m(a, q, axis, out, overwrite_input, method, keepdims, interpolation)\u001b[0m\n\u001b[0;32m   4281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _quantile_is_valid(q):\n\u001b[0;32m   4282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPercentiles must be in the range [0, 100]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 4283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_quantile_unchecked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4284\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\11 PrO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:4555\u001b[0m, in \u001b[0;36m_quantile_unchecked\u001b[1;34m(a, q, axis, out, overwrite_input, method, keepdims)\u001b[0m\n\u001b[0;32m   4547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantile_unchecked\u001b[39m(a,\n\u001b[0;32m   4548\u001b[0m                         q,\n\u001b[0;32m   4549\u001b[0m                         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4552\u001b[0m                         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   4553\u001b[0m                         keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   4554\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4556\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_quantile_ureduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4557\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4558\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4559\u001b[0m \u001b[43m                    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4560\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4561\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4562\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\11 PrO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:3823\u001b[0m, in \u001b[0;36m_ureduce\u001b[1;34m(a, func, keepdims, **kwargs)\u001b[0m\n\u001b[0;32m   3820\u001b[0m             index_out \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, ) \u001b[38;5;241m*\u001b[39m nd\n\u001b[0;32m   3821\u001b[0m             kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out[(\u001b[38;5;28mEllipsis\u001b[39m, ) \u001b[38;5;241m+\u001b[39m index_out]\n\u001b[1;32m-> 3823\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\11 PrO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:4721\u001b[0m, in \u001b[0;36m_quantile_ureduce_func\u001b[1;34m(a, q, axis, out, overwrite_input, method)\u001b[0m\n\u001b[0;32m   4719\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4720\u001b[0m         arr \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m-> 4721\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_quantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4722\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4723\u001b[0m \u001b[43m                   \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4724\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4725\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\11 PrO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:4823\u001b[0m, in \u001b[0;36m_quantile\u001b[1;34m(arr, quantiles, axis, method, out)\u001b[0m\n\u001b[0;32m   4819\u001b[0m previous_indexes, next_indexes \u001b[38;5;241m=\u001b[39m _get_indexes(arr,\n\u001b[0;32m   4820\u001b[0m                                               virtual_indexes,\n\u001b[0;32m   4821\u001b[0m                                               values_count)\n\u001b[0;32m   4822\u001b[0m \u001b[38;5;66;03m# --- Sorting\u001b[39;00m\n\u001b[1;32m-> 4823\u001b[0m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4825\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mprevious_indexes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4826\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mnext_indexes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4827\u001b[0m \u001b[43m                              \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4828\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m supports_nans:\n\u001b[0;32m   4830\u001b[0m     slices_having_nans \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(arr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def remove_outliers_iqr(data1, threshold=1.5):\n",
    "    \"\"\"\n",
    "    Remove outliers from a data1set using the Interquartile Range (IQR) method.\n",
    "    \n",
    "    Parameters:\n",
    "        data1 (numpy array or pandas data1Frame): The data1set.\n",
    "        threshold (float): The threshold value for identifying outliers. Default is 1.5.\n",
    "    \n",
    "    Returns:\n",
    "        numpy array or pandas data1Frame: The data1set with outliers removed.\n",
    "    \"\"\"\n",
    "    # Calculate the first quartile (Q1) and third quartile (Q3)\n",
    "    Q1 = np.percentile(data1, 25)\n",
    "    Q3 = np.percentile(data1, 75)\n",
    "    \n",
    "    # Calculate the interquartile range (IQR)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define the outlier cutoff (any data1 point outside this range is considered an outlier)\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    \n",
    "    # Find outliers\n",
    "    outliers = (data1 < lower_bound) | (data1 > upper_bound)\n",
    "    \n",
    "    # Remove outliers\n",
    "    cleaned_data1 = data1[~outliers]\n",
    "    \n",
    "    return cleaned_data1\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'data1' is your data1set (a numpy array or pandas data1Frame)\n",
    "cleaned_data1 = remove_outliers_iqr(data1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values and fill them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values in train set: ['Arrival Delay in Minutes']\n",
      "Columns with missing values in test set: ['Arrival Delay in Minutes']\n"
     ]
    }
   ],
   "source": [
    "def columns_with_missing_values(df):\n",
    "    # Check for missing values in each column\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    # Filter out columns with missing values\n",
    "    missing_columns = missing_values[missing_values > 0]\n",
    "\n",
    "    if missing_columns.empty:\n",
    "        return []\n",
    "    else:\n",
    "        return missing_columns.index.tolist()\n",
    "\n",
    "\n",
    "columns_with_missing = columns_with_missing_values(data1)\n",
    "print(\"Columns with missing values in train set:\", columns_with_missing)\n",
    "columns_with_missing = columns_with_missing_values(data2)\n",
    "print(\"Columns with missing values in test set:\", columns_with_missing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame without null values and duplicate rows in the specified column:\n",
      "            id CustomerGender CustomerType  Age TravelPurpose  ClassTravelled  \\\n",
      "0        70172           Male     Frequent   13    Recreation  PremiumEconomy   \n",
      "1         5047           Male   Occasional   25      Business        Business   \n",
      "2       110028         Female     Frequent   26      Business        Business   \n",
      "3        24026         Female     Frequent   25      Business        Business   \n",
      "6        82113           Male     Frequent   47    Recreation         Economy   \n",
      "...        ...            ...          ...  ...           ...             ...   \n",
      "99898    79335         Female     Frequent   40      Business        Business   \n",
      "100909    5921           Male     Frequent   41    Recreation         Economy   \n",
      "101096  113935         Female     Frequent   52      Business        Business   \n",
      "101920    5702         Female     Frequent    7    Recreation         Economy   \n",
      "102609   66787         Female     Frequent   64    Recreation         Economy   \n",
      "\n",
      "        DistanceToDestination  Inflight wifi  Time convenience  \\\n",
      "0                         460              3                 4   \n",
      "1                         235              3                 2   \n",
      "2                        1142              2                 2   \n",
      "3                         562              2                 5   \n",
      "6                        1276              2                 4   \n",
      "...                       ...            ...               ...   \n",
      "99898                    3911              5                 5   \n",
      "100909                     67              2                 2   \n",
      "101096                   1838              1                 1   \n",
      "101920                    234              1                 5   \n",
      "102609                   3711              3                 3   \n",
      "\n",
      "        WebsiteExperience  ...  Inflight entertainment system  \\\n",
      "0                       3  ...                              5   \n",
      "1                       3  ...                              1   \n",
      "2                       2  ...                              5   \n",
      "3                       5  ...                              2   \n",
      "6                       2  ...                              2   \n",
      "...                   ...  ...                            ...   \n",
      "99898                   5  ...                              3   \n",
      "100909                  2  ...                              3   \n",
      "101096                  1  ...                              5   \n",
      "101920                  0  ...                              4   \n",
      "102609                  3  ...                              2   \n",
      "\n",
      "        On-board service  Leg room in flight  Baggage handling ease  \\\n",
      "0                      4                   3                      4   \n",
      "1                      1                   5                      3   \n",
      "2                      4                   3                      4   \n",
      "3                      2                   5                      3   \n",
      "6                      3                   3                      4   \n",
      "...                  ...                 ...                    ...   \n",
      "99898                  4                   4                      4   \n",
      "100909                 5                   1                      2   \n",
      "101096                 3                   4                      5   \n",
      "101920                 4                   5                      1   \n",
      "102609                 2                   2                      2   \n",
      "\n",
      "        Checkin service  Inflight service  Cleanliness  \\\n",
      "0                     4                 5            5   \n",
      "1                     1                 4            1   \n",
      "2                     4                 4            5   \n",
      "3                     1                 4            2   \n",
      "6                     3                 5            2   \n",
      "...                 ...               ...          ...   \n",
      "99898                 4                 4            4   \n",
      "100909                3                 2            3   \n",
      "101096                4                 3            4   \n",
      "101920                4                 2            4   \n",
      "102609                1                 5            1   \n",
      "\n",
      "        Departure Delay in Minutes  Arrival Delay in Minutes  \\\n",
      "0                               25                      18.0   \n",
      "1                                1                       6.0   \n",
      "2                                0                       0.0   \n",
      "3                               11                       9.0   \n",
      "6                                9                      23.0   \n",
      "...                            ...                       ...   \n",
      "99898                          371                     339.0   \n",
      "100909                         314                     352.0   \n",
      "101096                         151                     385.0   \n",
      "101920                         724                     705.0   \n",
      "102609                         358                     369.0   \n",
      "\n",
      "        CustomerHappiness  \n",
      "0                 Unhappy  \n",
      "1                 Unhappy  \n",
      "2                   Happy  \n",
      "3                 Unhappy  \n",
      "6                 Unhappy  \n",
      "...                   ...  \n",
      "99898               Happy  \n",
      "100909            Unhappy  \n",
      "101096              Happy  \n",
      "101920            Unhappy  \n",
      "102609            Unhappy  \n",
      "\n",
      "[456 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming data1 is your DataFrame\n",
    "column_name = 'Arrival Delay in Minutes'\n",
    "\n",
    "# Create a SimpleImputer and fit it to your data\n",
    "imputer = SimpleImputer(strategy='mean')  # You can choose a different strategy if needed\n",
    "imputer.fit(data1[[column_name]])\n",
    "\n",
    "# Use the imputer to fill missing values in the specified column\n",
    "data1[column_name] = imputer.transform(data1[[column_name]])\n",
    "\n",
    "# Remove duplicate rows based on the specified column\n",
    "data1_no_duplicates = data1.drop_duplicates(subset=[column_name])\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(\"DataFrame without null values and duplicate rows in the specified column:\")\n",
    "print(data1_no_duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame without null values and duplicate rows in the specified column:\n",
      "           id CustomerGender CustomerType  Age TravelPurpose ClassTravelled  \\\n",
      "0       19556         Female     Frequent   52      Business        Economy   \n",
      "1       90035         Female     Frequent   36      Business       Business   \n",
      "3       77959           Male     Frequent   44      Business       Business   \n",
      "4       36875         Female     Frequent   49      Business        Economy   \n",
      "7       97286         Female     Frequent   43      Business       Business   \n",
      "...       ...            ...          ...  ...           ...            ...   \n",
      "25218   60214         Female   Occasional   43      Business       Business   \n",
      "25289  107748           Male     Frequent   66    Recreation        Economy   \n",
      "25356   13844         Female     Frequent   10    Recreation        Economy   \n",
      "25574   16776           Male     Frequent   63    Recreation        Economy   \n",
      "25771   15343           Male   Occasional   44      Business       Business   \n",
      "\n",
      "       DistanceToDestination  Inflight wifi  Time convenience  \\\n",
      "0                        160              5                 4   \n",
      "1                       2863              1                 1   \n",
      "3                       3377              0                 0   \n",
      "4                       1182              2                 3   \n",
      "7                       2556              2                 2   \n",
      "...                      ...            ...               ...   \n",
      "25218                   1546              4                 5   \n",
      "25289                    405              4                 1   \n",
      "25356                    594              2                 4   \n",
      "25574                    569              4                 5   \n",
      "25771                    331              2                 2   \n",
      "\n",
      "       WebsiteExperience  ...  ComfortOfSeats  Inflight entertainment system  \\\n",
      "0                      3  ...               3                              5   \n",
      "1                      3  ...               5                              4   \n",
      "3                      0  ...               4                              1   \n",
      "4                      4  ...               2                              2   \n",
      "7                      2  ...               5                              4   \n",
      "...                  ...  ...             ...                            ...   \n",
      "25218                  5  ...               4                              3   \n",
      "25289                  4  ...               4                              3   \n",
      "25356                  2  ...               3                              2   \n",
      "25574                  4  ...               4                              3   \n",
      "25771                  2  ...               5                              5   \n",
      "\n",
      "       On-board service  Leg room in flight  Baggage handling ease  \\\n",
      "0                     5                   5                      5   \n",
      "1                     4                   4                      4   \n",
      "3                     1                   1                      1   \n",
      "4                     2                   2                      2   \n",
      "7                     4                   4                      4   \n",
      "...                 ...                 ...                    ...   \n",
      "25218                 3                   5                      5   \n",
      "25289                 5                   3                      2   \n",
      "25356                 3                   1                      5   \n",
      "25574                 4                   4                      5   \n",
      "25771                 2                   4                      5   \n",
      "\n",
      "       Checkin service  Inflight service  Cleanliness  \\\n",
      "0                    2                 5            5   \n",
      "1                    3                 4            5   \n",
      "3                    3                 1            4   \n",
      "4                    4                 2            4   \n",
      "7                    5                 4            3   \n",
      "...                ...               ...          ...   \n",
      "25218                4                 5            4   \n",
      "25289                4                 1            4   \n",
      "25356                3                 4            3   \n",
      "25574                4                 3            4   \n",
      "25771                5                 5            5   \n",
      "\n",
      "       Departure Delay in Minutes  Arrival Delay in Minutes  \n",
      "0                              50                      44.0  \n",
      "1                               0                       0.0  \n",
      "3                               0                       6.0  \n",
      "4                               0                      20.0  \n",
      "7                              77                      65.0  \n",
      "...                           ...                       ...  \n",
      "25218                         291                     277.0  \n",
      "25289                         241                     246.0  \n",
      "25356                         128                     233.0  \n",
      "25574                         332                     311.0  \n",
      "25771                         295                     288.0  \n",
      "\n",
      "[321 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming data1 is your DataFrame\n",
    "column_name = 'Arrival Delay in Minutes'\n",
    "\n",
    "# Create a SimpleImputer and fit it to your data\n",
    "imputer = SimpleImputer(strategy='mean')  # You can choose a different strategy if needed\n",
    "imputer.fit(data2[[column_name]])\n",
    "\n",
    "# Use the imputer to fill missing values in the specified column\n",
    "data2[column_name] = imputer.transform(data2[[column_name]])\n",
    "\n",
    "# Remove duplicate rows based on the specified column\n",
    "data2_no_duplicates = data2.drop_duplicates(subset=[column_name])\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(\"DataFrame without null values and duplicate rows in the specified column:\")\n",
    "print(data2_no_duplicates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=4)\n",
    "\n",
    "data1['Arrival Delay in Minutes'] = imputer.fit_transform(data1[['Arrival Delay in Minutes']])\n",
    "data2['Arrival Delay in Minutes'] = imputer.fit_transform(data2[['Arrival Delay in Minutes']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "impute=KNNImputer()\n",
    "\n",
    "for i in  df_train.select_dtypes(include=\"number\").columns:\n",
    "    data1[i]=impute.fit_transform(data1[[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in  df_test.select_dtypes(include=\"number\").columns:\n",
    "    data2[i]=impute.fit_transform(data2[[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['Arrival Delay in Minutes']:\n",
    "    data1[i].fillna(data1[i].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['Arrival Delay in Minutes']:\n",
    "    data2[i].fillna(data2[i].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: ['CustomerGender', 'CustomerType', 'TravelPurpose', 'ClassTravelled', 'CustomerHappiness']\n"
     ]
    }
   ],
   "source": [
    "# fetch categorical features\n",
    "def get_categorical_features(df):\n",
    "    # Select columns with dtype 'object' (strings) or 'category'\n",
    "    categorical_features = data1.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    return categorical_features\n",
    "\n",
    "categorical_features = get_categorical_features(data1)\n",
    "print(\"Categorical features:\", categorical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODING THE FEATURES\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encode_categorical_features(df, categorical_features):\n",
    "    # Create a label encoder object\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Iterate through each categorical feature and encode its values\n",
    "    for feature in categorical_features:\n",
    "        data1[feature] = label_encoder.fit_transform(data1[feature])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Get categorical features\n",
    "categorical_features = get_categorical_features(data1)\n",
    "\n",
    "# Label encode categorical features\n",
    "data1 = label_encode_categorical_features(data1, categorical_features)\n",
    "# Now, categorical features in df have been label encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: ['CustomerGender', 'CustomerType', 'TravelPurpose', 'ClassTravelled']\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "# fetch categorical features\n",
    "def get_categorical_features(df):\n",
    "    # Select columns with dtype 'object' (strings) or 'category'\n",
    "    categorical_features = data2.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    return categorical_features\n",
    "\n",
    "categorical_features = get_categorical_features(data2)\n",
    "print(\"Categorical features:\", categorical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODING THE FEATURES\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encode_categorical_features(df, categorical_features):\n",
    "    # Create a label encoder object\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Iterate through each categorical feature and encode its values\n",
    "    for feature in categorical_features:\n",
    "        df[feature] = label_encoder.fit_transform(df[feature])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Get categorical features\n",
    "categorical_features = get_categorical_features(data2)\n",
    "\n",
    "# Label encode categorical features\n",
    "df2 = label_encode_categorical_features(data2, categorical_features)\n",
    "# Now, categorical features in df have been label encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 #test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'data' is your dataset (a pandas DataFrame or numpy array)\n",
    "\n",
    "# Create a box plot to visualize the distribution of each feature\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(df1)\n",
    "plt.xticks(range(1, len(df1.columns) + 1), df1.columns)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Box Plot of Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values: []\n"
     ]
    }
   ],
   "source": [
    "def columns_with_missing_values(data2):\n",
    "    # Check for missing values in each column\n",
    "    missing_values = data1.isnull().sum()\n",
    "\n",
    "    # Filter out columns with missing values\n",
    "    missing_columns = missing_values[missing_values > 0]\n",
    "\n",
    "    if missing_columns.empty:\n",
    "        return []\n",
    "    else:\n",
    "        return missing_columns.index.tolist()\n",
    "\n",
    "\n",
    "columns_with_missing = columns_with_missing_values(df2)\n",
    "print(\"Columns with missing values:\", columns_with_missing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### allocating X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the dataset into features (X) and the target variable (y)\n",
    "X = data1.drop(columns=['id','CustomerHappiness'])  # Features\n",
    "y = data1['CustomerHappiness']  # Target variable\n",
    "#X, data2 = X.align(data2, join='outer', axis=1, fill_value=0)\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X1 = df2.drop(columns=['id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Features:\n",
      "[[ 1.01503056 -0.4727667  -1.7452793  ...  1.30586973  0.26639265\n",
      "   0.07301421]\n",
      " [ 1.01503056  2.11520819 -0.95136024 ... -1.74229153 -0.36137482\n",
      "  -0.23753899]\n",
      " [-0.98519201 -0.4727667  -0.88520032 ...  1.30586973 -0.3875318\n",
      "  -0.39281559]\n",
      " ...\n",
      " [ 1.01503056  2.11520819 -0.62056063 ...  0.54382941 -0.20443295\n",
      "  -0.03050353]\n",
      " [-0.98519201  2.11520819 -1.14984    ... -1.74229153 -0.3875318\n",
      "  -0.39281559]\n",
      " [ 1.01503056 -0.4727667  -0.8190404  ... -1.74229153 -0.3875318\n",
      "  -0.39281559]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Reshape to a two-dimensional array\n",
    "# Initialize StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Reshape to a two-dimensional array\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Normalize the features (X)\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)  # Reshape y to a 2D array\n",
    "X_scaled1 =scaler.fit_transform(X1)\n",
    "# Display the normalized features (X)\n",
    "print(\"Normalized Features:\")\n",
    "print(X_scaled)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Replace existing X and y features in df1 with scaled features\n",
    "data1[X.columns] = X_scaled\n",
    "df2[X1.columns] = X_scaled1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a DataFrame named 'data'\n",
    "# If not, replace it with your DataFrame containing numerical columns\n",
    "\n",
    "# Generate a correlation matrix\n",
    "correlation_matrix = data1.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(30,40))\n",
    "\n",
    "# Plot the heatmap using Seaborn\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run this pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#X = data1.drop(columns=['CustomerHappiness'])  # Features\n",
    "y = data1['CustomerHappiness']  # Target variable\n",
    "X, df2 = X.align(df2, join='outer', axis=1)\n",
    "# Step 2: Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smapling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming act_df is your DataFrame with 'attack_cat' column representing class attack_cats\n",
    "\n",
    "# Check the unique values in the 'attack_cat' column\n",
    "unique_classes = data1['CustomerHappiness'].unique()\n",
    "\n",
    "# Print the unique classes\n",
    "print(\"Unique Classes:\", unique_classes)\n",
    "\n",
    "# Check the number of records for each class\n",
    "class_counts = data1['CustomerHappiness'].value_counts()\n",
    "print(\"Class Counts:\")\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming act_df is your DataFrame with 'attack_cat' column representing class attack_cats\n",
    "\n",
    "# Function to sample 50,000 rows from each group without replacement if the group size is sufficient\n",
    "def sample_rows(group):\n",
    "    sample_size = min(50000, len(group))\n",
    "    return group.sample(n=sample_size, replace=sample_size < 50000, random_state=0)\n",
    "\n",
    "# Apply the sampling function to each group (label)\n",
    "sampled_df = data1.groupby('CustomerHappiness', group_keys=False).apply(sample_rows)\n",
    "\n",
    "# Reset the index of the sampled DataFrame\n",
    "sampled_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the sampled DataFrame\n",
    "print(sampled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = sampled_df[sampled_df.duplicated()]\n",
    "print(\"Duplicate rows:\")\n",
    "print(duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming sampled_df is your DataFrame\n",
    "# Remove entirely identical rows\n",
    "sampled_df = sampled_df.drop_duplicates()\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "sampled_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the DataFrame without duplicates\n",
    "print(sampled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the dataset into features (X) and the target variable (y)\n",
    "X = sampled_df.drop(columns=['CustomerHappiness'])  # Features\n",
    "y = sampled_df['CustomerHappiness']  # Target variable\n",
    "X, data2 = X.align(data2, join='outer', axis=1, fill_value=0)\n",
    "# Step 2: Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Define the base classifiers\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "xgb_clf = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define the VotingClassifier\n",
    "super_clf1 = VotingClassifier(estimators=[('rf', rf_clf), ('xgb', xgb_clf)], voting='hard')\n",
    "\n",
    "# Train the super classifier\n",
    "super_clf1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = super_clf1.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Define the individual classifiers\n",
    "clf1 = DecisionTreeClassifier(random_state=42)\n",
    "clf2 = KNeighborsClassifier()\n",
    "clf3 = SVC(probability=True)\n",
    "\n",
    "# Define the ensemble classifier using hard voting\n",
    "voting_clf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)], voting='hard')\n",
    "\n",
    "# Train the ensemble classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import xgboost  as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Generate some random data for demonstration purposes\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "\n",
    "# Create an XGBoost model with adjusted parameters\n",
    "model1 = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,  # Adjusted this parameter\n",
    "    gamma=0.01,  # Removed gamma for simplicity, you can experiment with it\n",
    "    random_state=42,\n",
    "    colsample_bytree=0.8,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=600,\n",
    "    subsample=0.8,\n",
    "    reg_alpha=0.75,  # Adjusted regularization parameters\n",
    "    reg_lambda=0.25\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model on the training data\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model1.predict(X_test)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print F1 score and classification report\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Define the XGBoost classifier\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "# Define the evaluation metric (F1 score in this case)\n",
    "scoring_metric = 'f1'\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, scoring=scoring_metric, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the final model with the best hyperparameters on the entire training set\n",
    "final_model = XGBClassifier(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Evaluate the final model using F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"Validation F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "# Initialize Extra Trees Classifier\n",
    "extra_trees_model = ExtraTreesClassifier(n_estimators=300, random_state=42, criterion='gini',max_depth=None,min_samples_split=5,min_samples_leaf=1,bootstrap=True,n_jobs=None)\n",
    "\n",
    "# Train the model\n",
    "extra_trees_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = extra_trees_model.predict(X_test)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Display F1 score and classification report\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Sample data (replace this with your dataset)\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Display F1 score and classification report\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "# Initialize Decision Tree Classifier\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = decision_tree_model.predict(X_test)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Display F1 score and classification report\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hardvoting=xg+lgb+extra trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test are your training and testing data\n",
    "\n",
    "# Initialize individual classifiers with best parameters\n",
    "etc_model = ExtraTreesClassifier(n_estimators=300, max_depth=None, min_samples_split=5, min_samples_leaf=1)\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.1)\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, num_leaves=31)\n",
    "\n",
    "# Create a voting classifier\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('etc', etc_model),\n",
    "    ('xgb', xgb_model),\n",
    "    ('lgb', lgb_model)\n",
    "], voting='hard')\n",
    "\n",
    "# Train the voting classifier\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "ensemble_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, ensemble_pred)\n",
    "\n",
    "print(\"F1 Score of Super Classifier (Hard Voting):\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter tuning of light gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Define the classifier\n",
    "lgb_model = lgb.LGBMClassifier()\n",
    "\n",
    "# Define the hyperparameters grid to search\n",
    "param_grid = {\n",
    "    'num_leaves': [20, 30, 40],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'min_child_samples': [5, 10, 20],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "# Define the scoring metric\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid, scoring=scorer, cv=5)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best F1 Score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "# Initialize base models\n",
    "base_models = [\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=200, random_state=42)),\n",
    "    ('xgboost', XGBClassifier(learning_rate=1.0, n_estimators=200, random_state=42))\n",
    "]\n",
    "\n",
    "# Initialize StackingClassifier with a meta-classifier (Random Forest in this case)\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=RandomForestClassifier(random_state=42))\n",
    "\n",
    "# Train the model on the training set\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_stacked = stacking_model.predict(X_test)\n",
    "\n",
    "# Calculate F1 score for the stacked model\n",
    "f1_stacked = f1_score(y_test, y_pred_stacked, average='weighted')\n",
    "\n",
    "# Display F1 score and classification report for the stacked model\n",
    "print(\"Stacked Model:\")\n",
    "print(f\"F1 Score: {f1_stacked:.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_stacked))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "# Initialize base models\n",
    "base_models = [\n",
    "    ('adaboost', AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=42)),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "]\n",
    "\n",
    "# Initialize StackingClassifier\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=RandomForestClassifier(random_state=42))\n",
    "\n",
    "# Train the model on the training set\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "\n",
    "# Calculate F1 score on the test set\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Display F1 score and classification report\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensemble with three classifiers+k fold+regulariztaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([     0,      1,      2,      3,      4,      5,      6,      7,      9,\\n           10,\\n       ...\\n       103890, 103891, 103892, 103893, 103895, 103897, 103898, 103900, 103901,\\n       103902],\\n      dtype='int32', length=83123)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Perform k-fold cross-validation\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m kf\u001b[38;5;241m.\u001b[39msplit(X):\n\u001b[1;32m---> 29\u001b[0m     X_train, X_test \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m]\u001b[49m, X[test_index]\n\u001b[0;32m     30\u001b[0m     y_train, y_test \u001b[38;5;241m=\u001b[39m y[train_index], y[test_index]\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Initialize stacking classifier\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\11 PrO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3902\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3901\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3902\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3904\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\11 PrO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6116\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6118\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\11 PrO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6175\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6174\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6177\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6178\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index([     0,      1,      2,      3,      4,      5,      6,      7,      9,\\n           10,\\n       ...\\n       103890, 103891, 103892, 103893, 103895, 103897, 103898, 103900, 103901,\\n       103902],\\n      dtype='int32', length=83123)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X, y are your features and target variable\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('xgb', xgb.XGBClassifier(n_estimators=100, max_depth=5, reg_alpha=0.1, reg_lambda=0.1)),\n",
    "    ('lgbm', lgb.LGBMClassifier(n_estimators=100, max_depth=5, reg_alpha=0.1, reg_lambda=0.1)),\n",
    "    ('etc', ExtraTreesClassifier(n_estimators=100, max_depth=5))\n",
    "]\n",
    "\n",
    "# Define meta classifier\n",
    "meta_model = xgb.XGBClassifier(n_estimators=100, max_depth=5)\n",
    "\n",
    "# Initialize empty lists to store F1 scores for each fold\n",
    "f1_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Initialize stacking classifier\n",
    "    stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "    \n",
    "    # Train stacking classifier\n",
    "    stacking_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = stacking_model.predict(X_test)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate average F1 score across all folds\n",
    "average_f1_score = np.mean(f1_scores)\n",
    "\n",
    "print(\"Average F1 Score of Stacking Classifier with {}-fold cross-validation: {:.4f}\".format(n_splits, average_f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define the LightGBM model for binary classification\n",
    "lgb_model = lgb.LGBMClassifier(objective='binary', metric='binary_logloss', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "\n",
    "# Calculate F1 Score on the Test Set\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "# Initialize CatBoost Classifier\n",
    "catboost_model = CatBoostClassifier(iterations=160, depth=10, learning_rate=1.0, loss_function='MultiClass', random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "catboost_model.fit(X_train, y_train, verbose=100)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = catboost_model.predict(X_test)\n",
    "\n",
    "# Calculate F1 score on the test set\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Display F1 score and classification report\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = submission.shape[1]\n",
    "\n",
    "# Display the number of columns\n",
    "print(\"Number of Columns:\", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = data2.columns.tolist()\n",
    "\n",
    "# Display the list of features\n",
    "print(\"List of Features:\", features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lgb_model.predict(df2)\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Drop duplicate rows\n",
    "df_no_duplicates = submission.drop_duplicates()\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(submission)\n",
    "\n",
    "print(\"\\nDataFrame without duplicate rows:\")\n",
    "print(df_no_duplicates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "num_rows = df_no_duplicates.shape[0]\n",
    "\n",
    "print(f\"Number of rows in the DataFrame: {num_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['CustomerHappiness'] = test_predictions\n",
    "data2['CustomerHappiness'] = data2['CustomerHappiness'].replace({0: 'Unhappy', 1: 'Happy'})\n",
    "#data2['CustomerHappiness'] = data2['CustomerHappiness'].replace({'Happy': 'Unhappy', 'Unhappy': 'Happy'})\n",
    "submission = data2.drop(columns=[ 'CustomerGender', 'CustomerType', 'Age', 'TravelPurpose', 'ClassTravelled', 'DistanceToDestination', 'Inflight wifi', 'Time convenience', 'WebsiteExperience', 'ConvenienceOfGate', 'Inflight Food', 'Online check-in', 'ComfortOfSeats', 'Inflight entertainment system', 'On-board service', 'Leg room in flight', 'Baggage handling ease', 'Checkin service', 'Inflight service', 'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes',])\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'submission' is your DataFrame and 'column_name' is the column you want to check\n",
    "column_name = 'id'\n",
    "\n",
    "\n",
    "# Check if there are duplicate values in the specified column\n",
    "duplicates = submission.duplicated(subset=[column_name])\n",
    "\n",
    "# Print rows with duplicate values in the specified column\n",
    "print(submission[duplicates])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming data1 is your DataFrame\n",
    "duplicate_rows = submission[submission.duplicated()]\n",
    "\n",
    "# Display duplicate rows\n",
    "print(\"Duplicate Rows:\")\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submit7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming data1 is your DataFrame and 'column_name' is the column with duplicate values\n",
    "column_name = 'id'\n",
    "\n",
    "# Remove rows with duplicate values in the specified column\n",
    "data1_no_duplicates = submission.drop_duplicates(subset=[column_name])\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(\"DataFrame without duplicate rows in the specified column:\")\n",
    "print(data1_no_duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id': submission.id.astype('Int32'),\n",
    "                       'CustomerHappiness': submission[\"CustomerHappiness\"]})\n",
    "\n",
    "# Step 3: Save the DataFrame to a CSV file\n",
    "output.to_csv('submission10.csv.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume you have your features (X) and labels (y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Set up parameters for LightGBM\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_error',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    'num_boost_round': 1000,  # You can adjust the number of boosting rounds\n",
    "}\n",
    "\n",
    "# Train the LightGBM model with early stopping\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=True\n",
    ")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "y_pred_class = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Plot the training and validation performance\n",
    "plt.plot(range(1, model.best_iteration + 1), model.eval_train()['binary_error'], label='Training Error')\n",
    "plt.plot(range(1, model.best_iteration + 1), model.eval_valid()['binary_error'], label='Validation Error')\n",
    "plt.xlabel('Boosting Round')\n",
    "plt.ylabel('Error')\n",
    "plt.title('LightGBM Model Training and Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume you have your features (X) and labels (y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an Extra Trees model with adjusted parameters\n",
    "model = ExtraTreesClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',  # Use 'sqrt' instead of 'auto'\n",
    "    bootstrap=False,\n",
    "    random_state=42,\n",
    "    class_weight=None,\n",
    "    ccp_alpha=0.0,\n",
    "    max_samples=None\n",
    ")\n",
    "\n",
    "# Lists to store training and validation errors\n",
    "train_errors = []\n",
    "valid_errors = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, 100):  # Adjust the number of epochs as needed\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Training error\n",
    "    train_pred = model.predict(X_train)\n",
    "    train_error = 1 - np.mean(train_pred == y_train)\n",
    "    train_errors.append(train_error)\n",
    "    \n",
    "    # Validation error\n",
    "    valid_pred = model.predict(X_test)\n",
    "    valid_error = 1 - np.mean(valid_pred == y_test)\n",
    "    valid_errors.append(valid_error)\n",
    "\n",
    "# Plotting the training and validation errors\n",
    "epochs = np.arange(1, 100)  # Adjust the number of epochs as needed\n",
    "plt.plot(train_errors, label='Training Error')\n",
    "plt.plot(valid_errors, label='Validation Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Training and Validation Errors over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume you have your features (X) and labels (y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an XGBoost model with adjusted parameters\n",
    "model = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,  # Adjusted this parameter\n",
    "    gamma=0.01,  # Removed gamma for simplicity, you can experiment with it\n",
    "    random_state=42,\n",
    "    colsample_bytree=0.8,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=150,\n",
    "    subsample=0.8,\n",
    "    reg_alpha=0.25,  # Adjusted regularization parameters\n",
    "    reg_lambda=0.25\n",
    ")\n",
    "\n",
    "# Specify the evaluation dataset\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping_rounds = 10\n",
    "eval_metric = [\"error\", \"logloss\"]\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_metric=eval_metric,\n",
    "    eval_set=eval_set,\n",
    "    early_stopping_rounds=early_stopping_rounds,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Plot the training and validation performance\n",
    "results = model.evals_result()\n",
    "epochs = len(results['validation_0']['error'])\n",
    "\n",
    "plt.plot(range(1, epochs + 1), results['validation_0']['error'], label='Training Error')\n",
    "plt.plot(range(1, epochs + 1), results['validation_1']['error'], label='Validation Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('XGBoost Model Training and Validation Error')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
